---
title: "PPCA0026 - Tarefa de Casa: Simulação, Poder e Decisão"
subtitle: "Aplicando os Conceitos da Semana 4"
author: "Ítalo Guimarães / Sílvio Júnior"
date: "2025-07-04"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 3
    theme: cosmo
    code-fold: show
    code-tools: true
---

## Introdução

Nesta tarefa, você aplicará os conceitos de simulação para entender as propriedades de testes estatísticos, implementará um teste não-paramétrico do zero, e aplicará um framework de teoria da decisão para analisar um problema de classificação.

**Instruções Gerais:**

* Este arquivo serve como seu template de resposta. Preencha as seções marcadas com seu código R, as saídas geradas, e suas análises textuais.
* **Entrega:** Envie dois arquivos: este `.qmd` completo e o arquivo `.html` auto-contido resultante.

---

## Problema 1: O Poder de um Teste - Um Estudo de Simulação

### Parte A: Poder vs. Tamanho do Efeito

```{r prob1a_power_function}
# Carregar pacotes necessários para a tarefa
library(tidyverse)
library(caret) # Para createDataPartition, se necessário
library(ISLR2) # Para o dataset Default

# 1. Crie sua função `calcular_poder` aqui.
calcular_poder <- function(n, d, sigma, num_sim = 1000, alpha = 0.05) {

  rejeicoes <- 0
  
  valor_critico <- qnorm(1 - alpha / 2)
  
  erro_padrao <- sigma / sqrt(n)

  
  for (i in 1:num_sim) {
    
    dados_simulados <- rnorm(n, mean = d, sd = sigma)
    
    media_amostral <- mean(dados_simulados)
    
    z_stat <- media_amostral / erro_padrao
    
    # Verificar se a estatística de teste cai na área de rejeição
    if (abs(z_stat) > valor_critico) {
      rejeicoes <- rejeicoes + 1
    }
  }
  
  # Calcular o poder: a proporção de vezes que a hipótese nula foi rejeitada
  poder <- rejeicoes / num_sim
  
  return(poder)
}
```

```{r prob1a_run_and_plot}
# 2. Defina os parâmetros e calcule o poder para cada tamanho de efeito
n_fixo <- 30
sigma_fixo <- 1
efeitos_d <- seq(0, 2, by = 0.1)

poderes_calculados <- sapply(efeitos_d, function(d) {
  calcular_poder(n = n_fixo, d = d, sigma = sigma_fixo)
})

# 3. Crie o gráfico de Poder vs. Tamanho do Efeito
# Criar um dataframe para o ggplot
dados_plotagem <- data.frame(
  tamanho_efeito = efeitos_d,
  poder = poderes_calculados
)

# Gerar o gráfico
ggplot(dados_plotagem, aes(x = tamanho_efeito, y = poder)) +
  geom_line(color = "red") + # Mudei para vermelho para diferenciar
  geom_point(color = "red") +
  labs(
    title = "Poder do Teste vs. Tamanho do Efeito",
    x = "Tamanho do Efeito (d)",
    y = "Poder Estatístico"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)
```

**Análise da Parte A:**

Podemos perceber quanto maior a distâncias entre as médias das distribuições causada pelo aumento do efeito, maior o poder do teste. Isso é esperado, pois um efeito maior torna mais fácil detectar uma diferença significativa entre as médias.

### Parte B: Poder vs. Tamanho da Amostra

```{r prob1b_run_and_plot}
# 1. Defina os parâmetros
d_fixo <- 0.5
sigma_fixo <- 1
amostras_n <- seq(10, 200, by = 10)

poderes_calculados <- sapply(amostras_n, function(n) {
  calcular_poder(n = n, d = d_fixo, sigma = sigma_fixo)
})

# 3. Crie o gráfico de Poder vs. Tamanho do Efeito
# Criar um dataframe para o ggplot
dados_plotagem <- data.frame(
  tamanho_n = amostras_n,
  poder = poderes_calculados
)

# Gerar o gráfico
ggplot(dados_plotagem, aes(x = tamanho_n, y = poder)) +
  geom_line(color = "red") + # Mudei para vermelho para diferenciar
  geom_point(color = "red") +
  labs(
    title = "Poder do Teste vs. Tamanho da amostra",
    x = "Tamanho da amostra (n)",
    y = "Poder Estatístico"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)
```

**Análise da Parte B:**

Da mesma forma, quando aumentamos o "n" (tamanho da amostra) o erro padrão diminui, deixando o teste mais sensível a detectar diferenças entre as médias. Assim, o poder do teste aumenta com o tamanho da amostra.

---

## Problema 2: Teste de Permutação para Correlação

### Parte A: Teste Clássico

```{r prob2a_classical_test}

teste_cor <- cor.test(mtcars$wt, mtcars$mpg)

print(teste_cor)

```

**Análise da Parte A:**

Pelos resultados, podemos observar uma correlação negativa entre o peso do carro e a eficiência de combustível, com um p-valor muito baixo indicando que essa correlação é estatisticamente significativa.

### Parte B e C: Teste de Permutação e Análise

```{r prob2c_solution}

obs_cor <- cor(mtcars$mpg, mtcars$wt)

paste("Correlação Observada (mpg vs wt):", obs_cor)

# ...

# 2. Implemente o loop de permutação

num_sim <- 5000

null_dist_cor <- numeric(num_sim)

for (i in 1:num_sim) {
 
  wt_emb <- sample(mtcars$wt)
  
  null_dist_cor[i] <- cor(mtcars$mpg, wt_emb)
}

# ...

# 3. Visualize a distribuição nula e calcule o p-valor

ggplot(data.frame(correlacoes = null_dist_cor), aes(x = correlacoes)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  geom_vline(xintercept = obs_cor, color = "red", linetype = "dashed") +
  labs(title = "Distribuição nula empírica de correlações",
       x = "Correlação (sob H0)",
       y = "Frequência") +
  theme_minimal()


# 4. Calule o p-valor bilateral
p_valor <- mean(abs(null_dist_cor) >= abs(obs_cor))
paste("P-valor bilateral:", p_valor*2)

# ...
```

**Análise da Parte C:**

Podemos observar que, sob a hipótese nula, a distribuição das correlações é simétrica e centrada em zero. O p-valor bilateral indica que a correlação observada é estatisticamente significativa, reforçando os resultados do teste clássico. Da forma calculada, nenhuma das correlações calculadas foram tao maiores (ou menores) que a observada, o que reforça a ideia de que a correlação é significativa. Tal valor reforça o p-valor do teste calulado anteiormente que é aproximadamente igual a zero.

---

## Problema 3: Análise de Sensibilidade e Decisões Ótimas

### Parte A: Análise de Sensibilidade Teórica

```{r prob3a_sensitivity_analysis}
# Cenário: C_I = 1,000,000; C_II = 5,000,000
# Você precisará da sua função `calcular_poder` do Problema 1.
library(scales)

# 1. Defina os tamanhos de efeito e outros parâmetros
efeitos <- c(pequeno = 0.2, medio = 0.5, grande = 1.0)
n_fixo_prob3 <- 60 # Tamanho de amostra especificado
sigma_fixo_prob3 <- 1

erro_padrao <- sigma_fixo_prob3 / sqrt(n_fixo_prob3)



custo_erro1 <- 1000000 
custo_erro2 <- 5000000

# calulo do risco

efeito= efeitos[[1]]
z=(efeitos[[1]] / erro_padrao)

calcular_risco <- function(alpha, z, custo1, custo2) {
  

  valor_critico <- qnorm(1 - alpha, mean = 0, sd = 1)
  
  
  beta <- pnorm(valor_critico, mean = z, sd = 1)
  
  risco_total <- (alpha * custo1) + (beta * custo2)
  
  return(risco_total)
}


# 2. Simule para encontrar o alpha ótimo para cada tamanho de efeito

alphas_para_testar <- seq(0.001, 0.50, by = 0.001)

efeito1= efeitos[[1]]
z1=(efeito1 / erro_padrao)

efeito2= efeitos[[2]]
z2=(efeito2 / erro_padrao)

efeito3= efeitos[[3]]
z3=(efeito3 / erro_padrao)


riscos_totais1 <- sapply(alphas_para_testar, calcular_risco,
                        z = z1,
                        custo1 = custo_erro1,
                        custo2 = custo_erro2)

riscos_totais2 <- sapply(alphas_para_testar, calcular_risco,
                        z = z2,
                        custo1 = custo_erro1,
                        custo2 = custo_erro2)

riscos_totais3 <- sapply(alphas_para_testar, calcular_risco,
                        z = z3,
                        custo1 = custo_erro1,
                        custo2 = custo_erro2)



tabela_riscos <- tibble(
  alpha = alphas_para_testar,
  risco1 = riscos_totais1,
  risco2 = riscos_totais2,
  risco3 = riscos_totais3
)

ponto_otimo1 <- tabela_riscos %>% filter(risco1 == min(risco1))
ponto_otimo2 <- tabela_riscos %>% filter(risco2 == min(risco2))
ponto_otimo3 <- tabela_riscos %>% filter(risco3 == min(risco3))

print(paste("Ponto ótimo para efeito pequeno: α =", round(ponto_otimo1$alpha, 3), "com risco mínimo =", dollar(ponto_otimo1$risco1)))
print(paste("Ponto ótimo para efeito médio: α =", round(ponto_otimo2$alpha, 3), "com risco mínimo =", dollar(ponto_otimo2$risco2)))
print(paste("Ponto ótimo para efeito grande: α =", round(ponto_otimo3$alpha, 3), "com risco mínimo =", dollar(ponto_otimo3$risco3)))

tabela_riscos_long <- data.frame(
  alpha = rep(tabela_riscos$alpha, 3),
  risco = c(tabela_riscos$risco1, tabela_riscos$risco2, tabela_riscos$risco3),
  tipo = factor(rep(c("Efeito = 0.2", "Efeito = 0.5", "Efeito = 1"), each = nrow(tabela_riscos)))
)

ponto_otimo1_corrigido <- data.frame(alpha = ponto_otimo1$alpha, risco = ponto_otimo1$risco1, tipo = "Efeito = 0.2")
ponto_otimo2_corrigido <- data.frame(alpha = ponto_otimo2$alpha, risco = ponto_otimo2$risco2, tipo = "Efeito = 0.5")
ponto_otimo3_corrigido <- data.frame(alpha = ponto_otimo3$alpha, risco = ponto_otimo3$risco3, tipo = "Efeito = 1")
pontos_otimos <- rbind(ponto_otimo1_corrigido, ponto_otimo2_corrigido, ponto_otimo3_corrigido)

grafico_risco <- ggplot(tabela_riscos_long, aes(x = alpha, y = risco, color = tipo)) +
  geom_line(size = 1) +
  geom_point(data = pontos_otimos, aes(x = alpha, y = risco, color = tipo), size = 4) +
  labs(
    title = "Análise de Risco por tipo de Efeito",
    subtitle = expression(paste("Risco(", alpha, ") = ", alpha, "C"["I"], " + ", beta, "(", alpha, ")C"["II"])),
    x = "Nível de Significância (α)",
    y = "Risco Total Esperado",
    color = "Tipo de Risco"
  ) +
  scale_y_continuous(labels = dollar) +
  scale_color_manual(values = c("Efeito = 0.2" = "blue", "Efeito = 0.5" = "green", "Efeito = 1" = "orange")) +
  theme_bw()

grafico_risco

#grafico_risco <- ggplot(tabela_riscos, aes(x = alpha)) +
#  geom_line(aes(y = risco1), color = "blue", size = 1) +
#  geom_line(aes(y = risco2), color = "green", size = 1) +
#  geom_line(aes(y = risco3), color = "orange", size = 1) +
#  geom_point(data = ponto_otimo1, aes(x = alpha, y = risco1), color = "blue", size = 4) +
#  geom_point(data = ponto_otimo2, aes(x = alpha, y = risco2), color = "green", size = 4) +
#  geom_point(data = ponto_otimo3, aes(x = alpha, y = risco3), color = "orange", size = 4) +
#  labs(title = "Análise de Risco por tipo de Efeito",
#       subtitle = expression(paste("Risco(", alpha, ") = ", alpha, "C"["I"], " + ", beta, "(", alpha, ")C"["II"])),
#       x = "Nível de Significância (α)",
#       y = "Risco Total Esperado") +
#  scale_y_continuous(labels = dollar) + 
#  theme_bw()

#print(grafico_risco)


```

**Análise da Parte A:**

No caso exemplificado, quando a distância é pequena, podemos ver que, como o custo do erro tipo 2 é 5 vezes o do tipo 1, quanto maior o alfa, maior o poder do teste, ou seja, menor o beta que está intimamente ligado ao custo do erro tipo 2. Assim, o ponto ótimo de alfa (0.5) é aquele que minimiza o risco total esperado, levando em consideração os custos associados a cada tipo de erro. Com o aumento do tamanho do efeito, o ponto ótimo de alfa diminui, pois o poder do teste aumenta, tornando mais fácil detectar diferenças significativas. Isso é esperado, pois com um efeito maior, a probabilidade de cometer um erro tipo 2 diminui.

### Parte B: Análise de Sensibilidade em um Contexto de Machine Learning

```{r prob3b_setup}

set.seed(2024)

treino_indices <- createDataPartition(Default$default, p = 0.7, list = FALSE)


treino<- as.data.frame(Default[treino_indices, ])

#modelo_logistico <- glm(default ~ student + balance + income, data = treino, family = "binomial")

# Modelo nulo (apenas intercepto)
modelo_nulo <- glm(default ~ 1, data = treino, family = "binomial")

# Fórmula do modelo completo
formula_completa <- default ~ student + balance + income

# 3. Executar a seleção de variáveis forward
# O argumento 'trace = 1' mostra cada passo do processo
selecao_forward <- step(
  modelo_nulo,
  scope = list(lower = formula(modelo_nulo), upper = formula_completa),
  direction = "forward",
  trace = 1
)
summary(selecao_forward)

```

```{r prob3b_analysis}
# 1. Criar conjunto de validação
validacao <- as.data.frame(Default[-treino_indices, ])

# 2. Usar o modelo selecionado para fazer previsões
prob_geral0 <- predict(selecao_forward, newdata = validacao, type = "response")

# 3. Classificar usando ponto de corte de 0.5
pred_geral <- ifelse(prob_geral0 > 0.5, "Yes", "No")

# 4. Matriz de confusão geral
cm_geral <- table(Observado = validacao$default, Previsto = pred_geral)
print("Matriz de Confusão Geral:")
print(cm_geral)

# 5. Identificar subgrupos no conjunto de validação
estudantes <- validacao[validacao$student == "Yes", ]
nao_estudantes <- validacao[validacao$student == "No", ]

n_estudantes <- nrow(estudantes)
n_nao_estudantes <- nrow(nao_estudantes)

print(paste("Número de estudantes na validação:", n_estudantes))
print(paste("Número de não-estudantes na validação:", n_nao_estudantes))

# 6. Previsões para cada subgrupo
prob_estudantes <- predict(selecao_forward, newdata = estudantes, type = "response")
pred_estudantes <- ifelse(prob_estudantes > 0.5, "Yes", "No")

prob_nao_estudantes <- predict(selecao_forward, newdata = nao_estudantes, type = "response")
pred_nao_estudantes <- ifelse(prob_nao_estudantes > 0.5, "Yes", "No")

# 7. Matrizes de confusão por subgrupo
cm_estudantes <- table(Observado = estudantes$default, Previsto = pred_estudantes)
cm_nao_estudantes <- table(Observado = nao_estudantes$default, Previsto = pred_nao_estudantes)

print("Matriz de Confusão - Estudantes:")
print(cm_estudantes)
print("Matriz de Confusão - Não-Estudantes:")
print(cm_nao_estudantes)

# 8. Função para calcular custo a partir da matriz de confusão
# Assumindo custos: FN (não detectar inadimplência) = $5000, FP (erro falso positivo) = $500
calcular_custo <- function(cm, custo_fn = 5000, custo_fp = 500) {
  # Estrutura da matriz: linhas = observado, colunas = previsto
  # cm[1,1] = TN (True Negative), cm[1,2] = FP (False Positive)
  # cm[2,1] = FN (False Negative), cm[2,2] = TP (True Positive)
  
  if (nrow(cm) == 2 && ncol(cm) == 2) {
    fn <- cm[2, 1]  # Falso Negativo
    fp <- cm[1, 2]  # Falso Positivo
    custo_total <- fn * custo_fn + fp * custo_fp
  } else {
    # Caso não haja exemplos de uma das classes
    custo_total <- 0
  }
  
  return(custo_total)
}

# 9. Calcular custos totais
custo_total_geral <- calcular_custo(cm_geral)
custo_total_estudantes <- calcular_custo(cm_estudantes)
custo_total_nao_estudantes <- calcular_custo(cm_nao_estudantes)

print(paste("Custo Total Geral: $", format(custo_total_geral, big.mark = ",")))
print(paste("Custo Total Estudantes: $", format(custo_total_estudantes, big.mark = ",")))
print(paste("Custo Total Não-Estudantes: $", format(custo_total_nao_estudantes, big.mark = ",")))

# 10. Calcular custo médio por pessoa para cada grupo
custo_medio_geral <- custo_total_geral / nrow(validacao)
custo_medio_estudantes <- custo_total_estudantes / n_estudantes
custo_medio_nao_estudantes <- custo_total_nao_estudantes / n_nao_estudantes

print(paste("Custo Médio Geral por pessoa: $", round(custo_medio_geral, 2)))
print(paste("Custo Médio Estudantes por pessoa: $", round(custo_medio_estudantes, 2)))
print(paste("Custo Médio Não-Estudantes por pessoa: $", round(custo_medio_nao_estudantes, 2)))

# 11. Análise de métricas de desempenho

# Função para calcular métricas
calcular_metricas <- function(cm) {
  if (nrow(cm) == 2 && ncol(cm) == 2) {
    tn <- cm[1, 1]
    fp <- cm[1, 2]
    fn <- cm[2, 1]
    tp <- cm[2, 2]
    
    sensibilidade <- tp / (tp + fn)  # Recall
    especificidade <- tn / (tn + fp)
    precisao <- tp / (tp + fp)
    acuracia <- (tp + tn) / (tp + tn + fp + fn)
    
    return(data.frame(
      Sensibilidade = round(sensibilidade, 3),
      Especificidade = round(especificidade, 3),
      Precisao = round(precisao, 3),
      Acuracia = round(acuracia, 3)
    ))
  } else {
    return(data.frame(
      Sensibilidade = NA,
      Especificidade = NA,
      Precisao = NA,
      Acuracia = NA
    ))
  }
}

# Calcular métricas para cada grupo
metricas_geral <- calcular_metricas(cm_geral)
metricas_estudantes <- calcular_metricas(cm_estudantes)
metricas_nao_estudantes <- calcular_metricas(cm_nao_estudantes)

print("Métricas de Desempenho:")
print("Geral:")
print(metricas_geral)
print("Estudantes:")
print(metricas_estudantes)
print("Não-Estudantes:")
print(metricas_nao_estudantes)

# 12. Criar tabela resumo
tabela_resumo <- data.frame(
  Grupo = c("Geral", "Estudantes", "Não-Estudantes"),
  N = c(nrow(validacao), n_estudantes, n_nao_estudantes),
  Custo_Total = c(custo_total_geral, custo_total_estudantes, custo_total_nao_estudantes),
  Custo_Medio = c(custo_medio_geral, custo_medio_estudantes, custo_medio_nao_estudantes),
  Sensibilidade = c(metricas_geral$Sensibilidade, metricas_estudantes$Sensibilidade, metricas_nao_estudantes$Sensibilidade),
  Especificidade = c(metricas_geral$Especificidade, metricas_estudantes$Especificidade, metricas_nao_estudantes$Especificidade),
  Acuracia = c(metricas_geral$Acuracia, metricas_estudantes$Acuracia, metricas_nao_estudantes$Acuracia)
)

print("Tabela Resumo da Análise de Sensibilidade:")
print(tabela_resumo)

# 13. Visualização dos resultados
# Gráfico de custo médio por grupo
grafico_custo <- ggplot(tabela_resumo, aes(x = Grupo, y = Custo_Medio, fill = Grupo)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste("$", round(Custo_Medio, 2))), vjust = -0.5) +
  labs(
    title = "Custo Médio por Pessoa por Grupo",
    x = "Grupo",
    y = "Custo Médio ($)",
    fill = "Grupo"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

print(grafico_custo)

# Gráfico de métricas de desempenho
metricas_long <- data.frame(
  Grupo = rep(c("Geral", "Estudantes", "Não-Estudantes"), 3),
  Metrica = rep(c("Sensibilidade", "Especificidade", "Acurácia"), each = 3),
  Valor = c(tabela_resumo$Sensibilidade, tabela_resumo$Especificidade, tabela_resumo$Acuracia)
)

grafico_metricas <- ggplot(metricas_long, aes(x = Grupo, y = Valor, fill = Metrica)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Métricas de Desempenho por Grupo",
    x = "Grupo",
    y = "Valor da Métrica",
    fill = "Métrica"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)

print(grafico_metricas)
```


**Análise e Recomendação da Parte B:**

Com base na análise de sensibilidade realizada no conjunto de validação do dataset Default, observamos diferença no desempenho do modelo entre os subgrupos de estudantes e não-estudantes, revelando ideias para melhorar a gestão de risco de crédito.

#### Análise dos Custos Médios

A comparação dos custos médios por pessoa revela disparidade entre os grupos:

- **Estudantes**: $166.86 por pessoa
- **Não-estudantes**: $95.74 por pessoa

Esta diferença de 74% ($71.12 a mais por estudante) indica que o modelo atual aparenta ser menos eficiente para o subgrupo de estudantes. Considerando que temos 884 estudantes no conjunto de validação, esta ineficiência representa um custo adicional de aproximadamente $62,850 comparado ao desempenho ideal.

#### Análise do Desempenho por Métricas

##### Sensibilidade (Detecção de Inadimplentes)
Pelos resultados, podemos observar que o modelo apresenta sensibilidade baixa para ambos os grupos:

- **Estudantes**: 29.3% (detecta apenas 12 de 41 inadimplentes)
- **Não-estudantes**: 31.0% (detecta apenas 18 de 58 inadimplentes)

A similaridade nestes valores (diferença de apenas 1.7 pontos percentuais) sugere que o problema não está na capacidade diferencial de detecção, mas sim na calibração do ponto de corte da definição se o cliente é inadimplente ou não, principalmente por conta do alto desbalanceamento da amostra.

##### Especificidade (Identificação Correta de Não-inadimplentes)
Por outro lado, ambos os grupos apresentam especificidade excelente:

- **Estudantes**: 99.4%
- **Não-estudantes**: 99.8%

Esta alta especificidade indica mais uma vez que o corte de 0,5 não é o ideal para uma amostra tão desbalanceada.

##### Análise das Taxas de Erro
**Taxa de Falsos Negativos (o erro mais custoso):**

- **Estudantes**: 70.7% (29 de 41 inadimplentes não detectados)
- **Não-estudantes**: 69.0% (40 de 58 inadimplentes não detectados)

**Taxa de Falsos Positivos:**

- **Estudantes**: 0.6% (5 de 843 não-inadimplentes classificados erroneamente)
- **Não-estudantes**: 0.2% (5 de 2057 não-inadimplentes classificados erroneamente)

#### Interpretação dos Resultados

##### Causa Raiz do Problema
O modelo de regressão logística selecionado inclui um coeficiente negativo significativo para `studentYes` (-0.7872), indicando que, mantendo o saldo constante, estudantes têm menor probabilidade predita de inadimplência. Outro problema é o desbalanceamento amostral geral (3\% para 97\% aproximadamente) trazendo, por consequência também um desbalanceamento entre os subgrupos, 4,6\% x 95,4\%, para estudantes e, 2,8\% x 97,2\%, para não-estudantes.

Por conta da situação, poderíamos pensar em algumas opções para melhorar o custo:

* **Ajuste do Threshold de Classificação geral:** Reduzir o ponto de corte do modelo;
* **Ajuste do Threshold de Classificação por Subgrupo:** Criar dois modelos distintos, um para estudantes e outro para não-estudantes, com pontos de corte ajustados individualmente, uma vez que, de fato, cada subgrupo apresenta características distintas;

Abaixo, será realizada uma simulação para verificar qual melhor ponto de corte considerando o modelo geral, ou seja, sem criar modelos distintos para cada subgrupo.

##### Impacto Financeiro
Com custos de $5,000 por falso negativo e $500 por falso positivo:

```{r simulacao_custo}
#primeiro será feito um vetor com valores de 0.1 a 0.5, com incrementos de 0.01 para a simulação do threshold
thresholds <- seq(0.01, 0.5, by = 0.01)
# Função para calcular custo total com base no threshold considerando o modelo geral
calcular_custo_threshold <- function(threshold) {
  # Previsões com o threshold atual
  pred_geral1 <- ifelse(prob_geral0 > threshold, "Yes", "No")
  
  # Matriz de confusão
  cm_geral1 <- table(Observado = validacao$default, Previsto = pred_geral1)
  
  #print(paste("Threshold:", threshold))
  #print(cm_geral1)
  
  # Calcular custo total
  custo_total1 <- calcular_custo(cm_geral1)
  
  return(custo_total1)
}

#aplicando a funcao 
custo_thresholds <- sapply(thresholds, calcular_custo_threshold)
#mostrando o valor do trashhold que minimiza o custo
min_custo_threshold <- thresholds[which.min(custo_thresholds)]
print((paste("Threshold que minimiza o custo:", round(min_custo_threshold, 2))))
#criando um gráfico para visualizar o custo em função do threshold
ggplot(data.frame(Threshold = thresholds, Custo = custo_thresholds), aes(x = Threshold, y = Custo)) +
  geom_line(color = "blue") +
  geom_point(color = "red", size = 2) +
  labs(title = "Custo Total em Função do Threshold",
       x = "Threshold",
       y = "Custo Total ($)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::dollar)

```





Podemos observar, pelo gráfico acima, que, como o custo do falso-positivo é muito menor que o custo do falso-negativo, o threshold ideal que minimizaria o custo total é de 0.1.

```{r simulacao2}
# 1. Criar conjunto de validação
validacao <- as.data.frame(Default[-treino_indices, ])

trashold_ideal <- 0.1

# 2. Usar o modelo selecionado para fazer previsões
prob_geral0 <- predict(selecao_forward, newdata = validacao, type = "response")

# 3. Classificar usando ponto de corte de 0.1
pred_geral <- ifelse(prob_geral0 > trashold_ideal, "Yes", "No")

# 4. Matriz de confusão geral
cm_geral <- table(Observado = validacao$default, Previsto = pred_geral)
print("Matriz de Confusão Geral:")
print(cm_geral)

# 5. Identificar subgrupos no conjunto de validação
estudantes <- validacao[validacao$student == "Yes", ]
nao_estudantes <- validacao[validacao$student == "No", ]

n_estudantes <- nrow(estudantes)
n_nao_estudantes <- nrow(nao_estudantes)

print(paste("Número de estudantes na validação:", n_estudantes))
print(paste("Número de não-estudantes na validação:", n_nao_estudantes))

# 6. Previsões para cada subgrupo
prob_estudantes <- predict(selecao_forward, newdata = estudantes, type = "response")
pred_estudantes <- ifelse(prob_estudantes > trashold_ideal, "Yes", "No")

prob_nao_estudantes <- predict(selecao_forward, newdata = nao_estudantes, type = "response")
pred_nao_estudantes <- ifelse(prob_nao_estudantes > trashold_ideal, "Yes", "No")

# 7. Matrizes de confusão por subgrupo
cm_estudantes <- table(Observado = estudantes$default, Previsto = pred_estudantes)
cm_nao_estudantes <- table(Observado = nao_estudantes$default, Previsto = pred_nao_estudantes)

print("Matriz de Confusão - Estudantes:")
print(cm_estudantes)
print("Matriz de Confusão - Não-Estudantes:")
print(cm_nao_estudantes)

# 8. Função para calcular custo a partir da matriz de confusão
# Assumindo custos: FN (não detectar inadimplência) = $5000, FP (erro falso positivo) = $500
calcular_custo <- function(cm, custo_fn = 5000, custo_fp = 500) {
  # Estrutura da matriz: linhas = observado, colunas = previsto
  # cm[1,1] = TN (True Negative), cm[1,2] = FP (False Positive)
  # cm[2,1] = FN (False Negative), cm[2,2] = TP (True Positive)
  
  if (nrow(cm) == 2 && ncol(cm) == 2) {
    fn <- cm[2, 1]  # Falso Negativo
    fp <- cm[1, 2]  # Falso Positivo
    custo_total <- fn * custo_fn + fp * custo_fp
  } else {
    # Caso não haja exemplos de uma das classes
    custo_total <- 0
  }
  
  return(custo_total)
}

# 9. Calcular custos totais
custo_total_geral <- calcular_custo(cm_geral)
custo_total_estudantes <- calcular_custo(cm_estudantes)
custo_total_nao_estudantes <- calcular_custo(cm_nao_estudantes)

print(paste("Custo Total Geral: $", format(custo_total_geral, big.mark = ",")))
print(paste("Custo Total Estudantes: $", format(custo_total_estudantes, big.mark = ",")))
print(paste("Custo Total Não-Estudantes: $", format(custo_total_nao_estudantes, big.mark = ",")))

# 10. Calcular custo médio por pessoa para cada grupo
custo_medio_geral <- custo_total_geral / nrow(validacao)
custo_medio_estudantes <- custo_total_estudantes / n_estudantes
custo_medio_nao_estudantes <- custo_total_nao_estudantes / n_nao_estudantes

print(paste("Custo Médio Geral por pessoa: $", round(custo_medio_geral, 2)))
print(paste("Custo Médio Estudantes por pessoa: $", round(custo_medio_estudantes, 2)))
print(paste("Custo Médio Não-Estudantes por pessoa: $", round(custo_medio_nao_estudantes, 2)))

# 11. Análise de métricas de desempenho

# Função para calcular métricas
calcular_metricas <- function(cm) {
  if (nrow(cm) == 2 && ncol(cm) == 2) {
    tn <- cm[1, 1]
    fp <- cm[1, 2]
    fn <- cm[2, 1]
    tp <- cm[2, 2]
    
    sensibilidade <- tp / (tp + fn)  # Recall
    especificidade <- tn / (tn + fp)
    precisao <- tp / (tp + fp)
    acuracia <- (tp + tn) / (tp + tn + fp + fn)
    
    return(data.frame(
      Sensibilidade = round(sensibilidade, 3),
      Especificidade = round(especificidade, 3),
      Precisao = round(precisao, 3),
      Acuracia = round(acuracia, 3)
    ))
  } else {
    return(data.frame(
      Sensibilidade = NA,
      Especificidade = NA,
      Precisao = NA,
      Acuracia = NA
    ))
  }
}

# Calcular métricas para cada grupo
metricas_geral <- calcular_metricas(cm_geral)
metricas_estudantes <- calcular_metricas(cm_estudantes)
metricas_nao_estudantes <- calcular_metricas(cm_nao_estudantes)

print("Métricas de Desempenho:")
print("Geral:")
print(metricas_geral)
print("Estudantes:")
print(metricas_estudantes)
print("Não-Estudantes:")
print(metricas_nao_estudantes)

# 12. Criar tabela resumo
tabela_resumo <- data.frame(
  Grupo = c("Geral", "Estudantes", "Não-Estudantes"),
  N = c(nrow(validacao), n_estudantes, n_nao_estudantes),
  Custo_Total = c(custo_total_geral, custo_total_estudantes, custo_total_nao_estudantes),
  Custo_Medio = c(custo_medio_geral, custo_medio_estudantes, custo_medio_nao_estudantes),
  Sensibilidade = c(metricas_geral$Sensibilidade, metricas_estudantes$Sensibilidade, metricas_nao_estudantes$Sensibilidade),
  Especificidade = c(metricas_geral$Especificidade, metricas_estudantes$Especificidade, metricas_nao_estudantes$Especificidade),
  Acuracia = c(metricas_geral$Acuracia, metricas_estudantes$Acuracia, metricas_nao_estudantes$Acuracia)
)

print("Tabela Resumo da Análise de Sensibilidade:")
print(tabela_resumo)

# 13. Visualização dos resultados
# Gráfico de custo médio por grupo
grafico_custo <- ggplot(tabela_resumo, aes(x = Grupo, y = Custo_Medio, fill = Grupo)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste("$", round(Custo_Medio, 2))), vjust = -0.5) +
  labs(
    title = "Custo Médio por Pessoa por Grupo",
    x = "Grupo",
    y = "Custo Médio ($)",
    fill = "Grupo"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

print(grafico_custo)

# Gráfico de métricas de desempenho
metricas_long <- data.frame(
  Grupo = rep(c("Geral", "Estudantes", "Não-Estudantes"), 3),
  Metrica = rep(c("Sensibilidade", "Especificidade", "Acurácia"), each = 3),
  Valor = c(tabela_resumo$Sensibilidade, tabela_resumo$Especificidade, tabela_resumo$Acuracia)
)

grafico_metricas <- ggplot(metricas_long, aes(x = Grupo, y = Valor, fill = Metrica)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Métricas de Desempenho por Grupo",
    x = "Grupo",
    y = "Valor da Métrica",
    fill = "Métrica"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)

print(grafico_metricas)
```

Por fim, após ajustar o trashold para 0.1, observamos uma redução no custo total, tando para o grupo de estudante quanto para o de não estudadante. Além disso, apesar de uma pequena queda na especificidade (que ainda se manteve dentro de 90%), a sensibilidade aumentou significativamente.


#### Recomendação Final

##### Estratégia Recomendada: Segmentação e Otimização Diferenciada

1. **Implementação Imediata - Ajuste de Thresholds:**

   - Reduzir o ponto de corte para valores próximos de 0.1.

2. **Desenvolvimento de Modelos Específicos:**

   - Se houver espaço para tal (as vezes manter 2 modelos é mais caro que somente 1), poder-se-ia desenvolver modelos separados para estudantes e não-estudantes, ajustando os pontos de corte individualmente para minimizar ainda mais custos em cada subgrupo, uma vez que possuem taxa de default diferentes.

3. **Monitoramento e Validação Contínua:**

   - Monitorar continuamente a taxa de default e ajustar os modelos e os pontos de corte, caso necessário.
   - Implementar sistemas de monitoramento separados por subgrupo.
   - Validar regularmente se as diferenças observadas persistem em novos dados (backtesting).
