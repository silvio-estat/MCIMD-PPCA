---
title: "PPCA0026 - Tarefa Final: Otimização Bayesiana de um Modelo de Tópicos (LDA)"
subtitle: "Integrando MCMC e Otimização de Hiperparâmetros"
author: "Sílvio Júnior / Ítalo Vinícius"
date: "2025-08-29"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 3
    theme: cosmo
    code-fold: show
    code-tools: true
---

## Introdução

Este ficheiro serve como o seu template de resposta. Preencha as secções marcadas com o seu código R, as saídas geradas, e as suas análises textuais.

```{r setup, include=FALSE}
# Carregue todos os pacotes que você usará aqui
library(tidyverse)
library(tidytext)
library(tm)
library(textmineR)
library(reticulate)

# Configure o reticulate para usar o seu ambiente Python, se necessário.
# Primeiro, vamos verificar qual Python o reticulate está detectando:
cat("Python detectado pelo reticulate:\n")
print(py_config())

# Se necessário, descomente e ajuste uma das linhas abaixo:
# Opção 1: Caminho específico
#use_python("C:\\Python311\\python.exe", required = TRUE)
# Opção 2: Usar Anaconda/Miniconda
#use_condaenv("base", required = TRUE)
# Opção 3: Usar ambiente virtual
#use_virtualenv("nome_do_ambiente", required = TRUE)
```

---

## Problema 1: Preparação dos Dados e Implementação do Sampler LDA

### Parte A: Preparação dos Dados

1.  **Obtenha e Processe os Dados:** Comece por carregar o ficheiro `cpi_pandemia_discursos.csv`.
2.  **Amostragem:** Crie um subconjunto com uma amostra aleatória de 500 discursos.
3.  **Pipeline de Pré-processamento:** Execute a limpeza de texto, crie uma DTM e filtre-a.

```{r prob1a_data_prep, message=FALSE, warning=FALSE}
# Verificar onde estamos e listar arquivos disponíveis
cat("Diretório atual:", getwd(), "\n")

# 1. Carregue os Dados
# Ajuste o caminho conforme necessário baseado na busca acima
discursos_cpi <- read_csv(paste(getwd(),"tarefa/6/cpi_pandemia_discursos.csv",sep="/"))

# 2. Amostragem de 500 discursos
set.seed(123) # Para reprodutibilidade

discursos_cpi_amostra <- discursos_cpi %>% 
  slice_sample(n = 500)

# 3. Pipeline de Pré-processamento

# 3.1. Tokenize o texto em palavras individuais e converta para minúsculas
discursos_tokens <- discursos_cpi_amostra %>%
  unnest_tokens(word, text) %>% 
  mutate(word = str_to_lower(word))

cat("Número de tokens antes da limpeza:", nrow(discursos_tokens), "\n")

# 3.2. Remova pontuação, números e stopwords
stopwords_pt <- get_stopwords(language = "pt")

#(PRECISA? COMO NÃO CONHEÇO DIREITO A AREA NÃO SEI FALAR SOBRE)
# Adicionar stopwords customizadas se necessário 
#stopwords_custom <- c("é", "são", "foi", "ser", "ter", "estar", "fazer", "vai", "vão", "pode", "podem")

discursos_limpos <- discursos_tokens %>%
  # Remover números e pontuação (manter apenas letras)
  filter(str_detect(word, "^[a-záàâãéèêíìîóòôõúùûç]+$")) %>%
  # Remover palavras muito curtas (menos de 3 caracteres)
  filter(nchar(word) >= 3) %>%
  # Remover stopwords em português
  anti_join(stopwords_pt, by = "word") #%>%
  # Remover stopwords customizadas
  #filter(!word %in% stopwords_custom)

cat("Número de tokens após limpeza:", nrow(discursos_limpos), "\n")

# 3.3. Criar Document-Term Matrix (DTM)
# Contar frequência de termos por documento
dtm_data <- discursos_limpos %>%
  count(doc_id, word, sort = TRUE) %>% 
  ungroup()

# Converter para formato de matriz esparsa
dtm_sparse <- dtm_data %>%
  cast_dtm(doc_id, word, n)

cat("Dimensões da DTM inicial:", dim(dtm_sparse), "\n")

# 3.4. Filtrar DTM para remover termos raros
# Remover termos que aparecem em menos de 5 documentos
term_freq <- dtm_data %>%
  group_by(word) %>%
  summarise(doc_count = n_distinct(doc_id), .groups = 'drop') %>%
  filter(doc_count >= 5)

# Filtrar DTM mantendo apenas termos frequentes
dtm_filtered_data <- dtm_data %>%
  semi_join(term_freq, by = "word")

# Criar DTM final filtrada
dtm_final <- dtm_filtered_data %>%
  cast_dtm(doc_id, word, n)

# Ao final, imprima as dimensões da sua DTM final
cat("Dimensões da DTM final (após filtros):", dim(dtm_final), "\n")
cat("Número de documentos:", nrow(dtm_final), "\n")
cat("Número de termos únicos:", ncol(dtm_final), "\n")

# Mostrar alguns termos mais frequentes
termos_frequentes <- dtm_filtered_data %>%
  group_by(word) %>%
  summarise(total_freq = sum(n), doc_count = n_distinct(doc_id), .groups = 'drop') %>%
  arrange(desc(total_freq)) %>%
  head(10)

cat("\nTop 10 termos mais frequentes:\n")
print(termos_frequentes)
```

**Análise da Parte 1.A:**

Primeiramente foi jeito o ajuste da pasta de trabalho para que independente da pessoa que vier realizar o "pull" do projeto do github consiga buscar o documento de interesse. Depois foi feita uma amostragem aleatória de 500 discursos conforme solicitado. Após a amostragem, o trabalho com os tokens dos discursos foi inicializado. 

Primeiro foi feito a tokenização fui guardada na variável "word" através do `unnest_tokens`. Após isso foi feito uma limpeza destes transformando-os em letras minúsculas, retirando palavras com caracteres estranhos, e retirando outras "stopwords" definidas em `stopwords_pt`. 

Após a primeira limpeza, iniciou-se a criação da matriz DTM sendo `cast_dtm` e retirou-se os tokens que tinham frequência absoluta abaixo de 5. Por fim foi feita a apresentação de alguns resultados como o número de documentos iqual a 37 (500 discursos) e 2059 tokens únicos.

### Parte B: Implementação e Estruturação do Gibbs Sampler

1.  **Crie uma Função `run_lda_sampler`:** Implemente o algoritmo Gibbs sampler para LDA.

```{r prob1b_sampler_function}
run_lda_sampler <- function(dtm, K, alpha, beta, num_iterations, burn_in) {

  # --- 1. Validação e Preparação dos Dados ---

  if (burn_in >= num_iterations) {
    stop("O número de iterações (num_iterations) deve ser maior que o período de burn-in.")
  }

  # Obter o vocabulário e seu tamanho
  vocab <- colnames(dtm)
  V <- length(vocab)
  num_docs <- nrow(dtm)

  # Mapear palavras para índices numéricos para eficiência
  word_to_id <- 1:V
  names(word_to_id) <- vocab

  # Converter a DTM para um formato de lista de IDs de palavras (corpus_ids).
  # Este formato é mais conveniente para o Gibbs Sampler, onde cada documento
  # é um vetor de IDs de palavras, com repetições.
  corpus_ids <- apply(dtm, 1, function(document_row) {
    # Pega apenas as palavras que aparecem no documento
    word_counts <- document_row[document_row > 0]
    
    if(length(word_counts) == 0) {
      return(integer(0))  # Documento vazio
    }
    
    # Repete o ID de cada palavra pelo número de vezes que ela aparece
    rep(word_to_id[names(word_counts)], times = word_counts)
  })

  #print("o corpus_ids é:")
  #print(head(corpus_ids))

  # --- 2. Inicialização Aleatória ---

  # `z` é uma lista que armazena a atribuição de tópico para cada instância de palavra
  set.seed(123) # Para reprodutibilidade
  z <- lapply(corpus_ids, function(doc) {
    if (length(doc) > 0) {
      sample(1:K, size = length(doc), replace = TRUE)
    } else {
      integer(0) # Documento vazio
    }
  })

  # Inicialização das matrizes de contagem
  # ndk: matriz documento-tópico
  ndk <- matrix(0, nrow = num_docs, ncol = K)
  # nkv: matriz tópico-palavra
  nkv <- matrix(0, nrow = K, ncol = V)
  # nk: vetor com o total de palavras por tópico
  nk <- rep(0, K)

  # Preencher as matrizes de contagem com base na inicialização aleatória
  for (d in 1:num_docs) {
    if (length(corpus_ids[[d]]) > 0) {
        for (i in 1:length(corpus_ids[[d]])) {
            topic <- z[[d]][i]
            word_id <- corpus_ids[[d]][i]

            ndk[d, topic] <- ndk[d, topic] + 1
            nkv[topic, word_id] <- nkv[topic, word_id] + 1
            nk[topic] <- nk[topic] + 1
        }
    }
  }

  # --- 3. O Loop do Gibbs Sampler ---

  cat("Iniciando o Gibbs Sampler...\n")

  for (iter in 1:num_iterations) {
    if (iter %% 100 == 0) {
      cat(paste("Iteração", iter, "de", num_iterations, "\n"))
    }

    # Loop sobre cada documento e cada palavra no documento
    for (d in 1:num_docs) {
      if (length(corpus_ids[[d]]) == 0) next # Pular documentos vazios

      for (i in 1:length(corpus_ids[[d]])) {
        word_id <- corpus_ids[[d]][i]
        old_topic <- z[[d]][i]

        # (1) Decrementar contagens para a palavra e tópico atuais
        ndk[d, old_topic] <- ndk[d, old_topic] - 1
        nkv[old_topic, word_id] <- nkv[old_topic, word_id] - 1
        nk[old_topic] <- nk[old_topic] - 1

        # (2) Calcular a probabilidade condicional para cada tópico
        # Esta é a equação central do Collapsed Gibbs Sampling para LDA
        p_topic_given_doc <- ndk[d, ] + alpha
        p_word_given_topic <- (nkv[, word_id] + beta) / (nk + V * beta)
        cond_prob <- p_topic_given_doc * p_word_given_topic

        # (3) Amostrar um novo tópico a partir da distribuição de probabilidade
        # A função sample normaliza internamente o vetor de probabilidades `prob`
        new_topic <- sample(1:K, size = 1, prob = cond_prob)

        # (4) Atualizar a atribuição e as contagens com o novo tópico
        z[[d]][i] <- new_topic
        ndk[d, new_topic] <- ndk[d, new_topic] + 1
        nkv[new_topic, word_id] <- nkv[new_topic, word_id] + 1
        nk[new_topic] <- nk[new_topic] + 1
      }
    }
  }
  
  cat("Gibbs Sampler concluído.\n")

  # --- 4. Preparação da Saída ---

  # Adicionar nomes às dimensões das matrizes para facilitar a interpretação
  rownames(ndk) <- rownames(dtm)
  colnames(ndk) <- paste0("Tópico_", 1:K)
  rownames(nkv) <- paste0("Tópico_", 1:K)
  colnames(nkv) <- vocab

  # Criar a lista de saída conforme especificado nos requisitos
  output <- list(
    ndk = ndk,
    nkv = nkv,
    vocab = vocab
  )

  return(output)
}

#testando se está rodando
#resultados_lda <- run_lda_sampler(
#  dtm = dtm_final,  # CORRIGIDO: usar dtm_final em vez de dtm_filtered_data
#  K = 6,
#  alpha = 0.2,
#  beta = 0.2,
#  num_iterations = 1000,
#  burn_in = 200
#)

#print(resultados_lda)

```

---

## Problema 2: Otimização Bayesiana dos Hiperparâmetros

### Parte A: Definição da Função Objetivo

1.  **Crie a Função Objetivo:** Escreva uma função em R que execute o sampler e calcule a coerência dos tópicos.

```{r prob2a_objective_function}
# Função para calcular a coerência (pode usar a do gabarito como referência)
calcular_coerencia <- function(nkv, dtm, vocab, num_top_words = 10) {
  # ... (código da função de coerência) ...
}

# Função Objetivo para a BO
objective_function_lda <- function(params) {
  alpha <- params[[1]]
  beta <- params[[2]]
  
  # ... seu código aqui ...
  # Chame run_lda_sampler e calcular_coerencia.
  # Lembre-se de retornar -1 * coerência.
}
```

### Parte B: Configuração e Execução da BO em Python

1.  **Defina o Espaço de Busca e Execute o Otimizador:**

```{python prob2b_bo_run}
#| echo: true
#| eval: true

from skopt import gp_minimize
from skopt.space import Real
from skopt.plots import plot_convergence, plot_objective
import matplotlib.pyplot as plt

# 1. Defina o Espaço de Busca para alpha e beta
# ... seu código aqui ...

# 2. Execute o Otimizador
# result_lda_bo = gp_minimize(...)

# Guarde os resultados para usar em R
# optimal_alpha = result_lda_bo.x[0]
# optimal_beta = result_lda_bo.x[1]
```

---

## Problema 3: Análise dos Resultados

### Parte A: Análise da Otimização

1.  **Reporte os Melhores Hiperparâmetros e Visualize os Resultados:**

```{r prob3a_analysis}
# Use o objeto `py` para aceder aos resultados do Python em R
# resultado_python <- py$result_lda_bo

# Reporte os valores ótimos
# ...
```{python prob3a_plots}
#| echo: true
#| eval: true

# Visualize a convergência e a superfície de resposta
# plot_convergence(...)
# plot_objective(...)
```

**Análise da Parte A:**

*SUA ANÁLISE AQUI:* Inclua os seus gráficos. O que eles mostram sobre o processo de otimização? A BO conseguiu encontrar uma região promissora no espaço de hiperparâmetros?

### Parte B: Interpretando o Modelo Final

1.  **Execute o Modelo Final e Apresente os Tópicos:**

```{r prob3b_final_model}
# 1. Execute o Modelo Final com mais iterações, usando os
#    hiperparâmetros ótimos encontrados.
# ... seu código aqui ...

# 2. Extraia os top 10 termos para cada tópico e apresente-os numa tabela.
# ... seu código aqui ...
```

**Análise da Parte B:**

*SUA ANÁLISE E TABELA AQUI:* Apresente a sua tabela de tópicos e atribua um "rótulo" interpretável a cada um. A otimização dos hiperparâmetros resultou em tópicos que parecem coerentes e distintos? Discuta a sua interpretação.

